class(temp)
temp[2,]
temp[,2]
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
sd_fit
sd_lwr
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
ssd_lwr
sd_lwr
sd_upr
sd_fit
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
attach(faithful)
eruption.lm <- lm(eruptions ~ waiting)
eruption.lm
fitted
newdata = data.frame(waiting=80)
eruption.lm = lm(eruptions ~ waiting)
eruptions
eruptions.lm
eruption.lm
predict(eruption.lm, newdata, interval = "predict")
eruptions
waiting
fitted = lm(waiting2 ~ duration2)
newdata = data.frame(waiting2=80)
predict(fitted, newdata, interval = "predict")
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
fitted
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
}
#BUGGGG
waiting_2 <- sapply(duration2, get_val)
diff_sqaured <- (waiting_2-waiting2)^2
#Ben Greenawald and Meredith Thomas
#Instructor: Heather Cook
###########################################
#Problem 1
#1
binom.test(2, 15, p=.5, alternative = "less")
#This outputs .003693, which is the probability of a type 1 error
#2
binom.test(2,15,p=.3, alternative = "less")
#Our probability of a type 2 error is .1268
#3
binom.test(2,15,p=.1, alternative = "less")
#Our probability of a type 2 error is .8159
###########################################
#Problem 2
#1
pnorm((.4-.5)/sqrt((.5*.5)/60))
#This outputs .06066, the type 1 error probability.
#2
pnorm((.4-.25)/sqrt(.25*.25/60))
#Our probability of a type 2 error is .9999983
###########################################
library(MASS)
attach(geyser)
#1
fitted = lm(waiting2 ~ duration2)
#plot(fitted)
#The intercept (waiting2) is 34.95 while the slope of the model line
#10.78
#2
#Using the residuals vs. fitted graph, we can see that within each of
#our blobs, the points seem to be distributed randomly, but symmetrically
#about the dotted line, so our data does not seem to violate linearity or
#homoscedasticity. But looking at the qqplot of the residuals, we see
#that the residuals are not normal at the extremes, and thus violate this
#requirement of linear models, thus we conclude that this is not a relaible
#model of the data
#3
summary(fitted)
#We are testing the assumption the B=0 (where B represents the slope of the
#line of best fit in waiting2 ~ duration2). We want to test at a significance
#level of .05. So Ho: B=0 while Ha: B!=0. Using the summary function,
#we see that our p-value is 2.2e-16 which is smaller than .05 so we reject
#our null and conclude that there is a linear relationship between
#waiting time and duration of eruption.
#4
#Way one, we will use the summary function and look at the value
summary(fitted)
#which tells us that R^2 is .7887
#Way two is to extract it directly from the summary function
summary(fitted)$adj.r.squared
#and we get the same result
#For the third way, we will write a function that uses our linear model to calculate r^2
get_val <- function(x){
y <- (10.78*x) + 34.95
return(y)
}
#BUGGGG
waiting_2 <- sapply(duration2, get_val)
diff_sqaured <- (waiting_2-waiting2)^2
dist <- (waiting2 - mean(waiting2))^2
sum_dist <- sum(dist)
total_line_error <- sum(diff_sqaured)
r_2 <- sum_dist / total_line_error
#4
newdta = data.frame(waiting2=3)
model <- predict(fitted, newdata=newdta, interval = "predict")
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
waiting2
class(waiting2)
class(duration2)
predict(fitted, interval = "predict")
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
model
duration2
getwd()
getwd()
Cars93
head(Cars93)
source('~/Documents/Spring 2016/STAT 3080/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Documents/Spring 2016/STAT 3080/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
head(Cars93)
model_0 <- lm(MPG.city ~ EngineSize+Weight+Passengers+Price, data = Cars93)
summary(model_0)
source('~/Documents/Spring 2016/STAT 3080/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
summary(model_1)
#We do the same with the next highest non-signficant p-value, Passengers
model_2 <- update(model_1, ~.-Passengers)
summary(model_2)
source('~/Documents/Spring 2016/STAT 3080/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
plot(model_3)
data <- read.delim("Burnout.dat", header = TRUE)
head(data)
library(psych)
dummy <- C(data$burnout, treatment)
dummy
model <- lm(data$burnout ~ data$loc + data$cope)
model
summary(model)
source('~/Documents/Spring 2016/STAT 3080/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
model_0 <- lm(log(MPG.city) ~ EngineSize+Weight+Passengers+Price, data = Cars93)
summary(model_0)
#and remove that variable from our model to create a new model.
model_1 <- update(model_0, ~.-EngineSize)
summary(model_1)
#We do the same with the next highest non-signficant p-value, Passengers
#with a p-value of .558
model_2 <- update(model_1, ~.-Passengers)
summary(model_2)
# We do the same with Price whose p-value is .257
model_3 <- update(model_2, ~.-Price)
summary(model_3)
#################
plot(model_3)
model_0 <- lm(log(log(MPG.city)) ~ EngineSize+Weight+Passengers+Price, data = Cars93)
summary(model_0)
#We do the same with the next highest non-signficant p-value, Passengers
#with a p-value of .558
model_2 <- update(model_1, ~.-Passengers)
summary(model_2)
#We will try and take the logarithm of the data to see if we get a better model
model_0 <- lm(log(log(MPG.city)) ~ EngineSize+Weight+Passengers+Price, data = Cars93)
summary(model_0)
#We do the same with the next highest non-signficant p-value, Passengers
#with a p-value of .558
model_2 <- update(model_0, ~.-Passengers)
summary(model_2)
#We choose the highest non-significant p-value (EngineSize with a p-value of .740)
#and remove that variable from our model to create a new model.
model_1 <- update(model_2, ~.-EngineSize)
summary(model_1)
# We do the same with Price whose p-value is .257
model_3 <- update(model_2, ~.-Price)
summary(model_3)
plot(model_3)
#1
pnorm((.4-.5)/sqrt((.5*.5)/60))
#This outputs .06066, the type 1 error probability.
#2
pnorm((.4-.25)/sqrt(.25*.25/60))
#Our probability of a type 2 error is .9999983
###########################################
library(MASS)
attach(geyser)
duration2<-duration[-299]
waiting2<-waiting[-1]
#1
fitted = lm(waiting2 ~ duration2)
#plot(fitted)
#The intercept (waiting2) is 34.95 while the slope of the model line
#10.78
#2
#Using the residuals vs. fitted graph, we can see that within each of
#our blobs, the points seem to be distributed randomly, but symmetrically
#about the dotted line, so our data does not seem to violate linearity or
#homoscedasticity. Looking at the qqplot of the residuals, we see
#that the residuals also appear to be approximately normal so our assumptions
#for linear models do hold.
#3
summary(fitted)
#We are testing the assumption the B=0 (where B represents the slope of the
#line of best fit in waiting2 ~ duration2). We want to test at a significance
#level of .05. So Ho: B=0 while Ha: B!=0. Using the summary function,
#we see that our p-value is 2.2e-16 which is smaller than .05 so we reject
#our null and conclude that there is a linear relationship between
#waiting time and duration of eruption.
#4
#Way one, we will use the summary function and look at the value
summary(fitted)
#which tells us that R^2 is .789
#Way two is to extract it directly from the summary function
summary(fitted)$adj.r.squared
#and we get the same result
#For the third way, we will write a function that uses our linear model to calculate r^2
get_val <- function(x){
y <- (10.78*x) + 34.95
return(y)
}
#Maybe a small bug?
waiting_2 <- sapply(duration2, get_val)
diff_sqaured <- (waiting_2-waiting2)^2
dist <- (waiting2 - mean(waiting2))^2
sum_dist <- sum(dist)
total_line_error <- sum(diff_sqaured)
r_2 <- total_line_error / sum_dist
1-r_2
#4,
new <- c(3,4,5)
predict.lm(fitted, newdata = new, interval = "prediction")
#4,
new <- c(3,4,5)
predict.lm(fitted, newdata = new, interval = "confidence")
#4,
new <- data.frame(3,4,5)
predict.lm(fitted, newdata = new, interval = "confidence")
#4,
new <- data.frame(3,4,5)
predict.lm(fitted, newdata = new, interval = "prediction")
#4,
new <- data.frame(waiting2 <- c(3,4,5))
predict.lm(fitted, newdata = new, interval = "prediction")
#4,
new <- data.frame(duration2 <- c(3,4,5))
predict.lm(fitted, newdata = new, interval = "prediction")
#4,
new <- data.frame(duration2 <- c(3,4,5))
pred <- predict.lm(fitted, newdata = new, interval = "prediction")
pred <- cbind(new, pred)
pred
#4,
new <- data.frame(duration2 <- c(3,4,5))
pred <- predict.lm(fitted, newdata = new, interval = "prediction")
pred <- cbind("names"= new, pred)
pred
new
#4,
new <- data.frame(duration2 <- c(3,4,5))
names(new) <- "names"
pred <- predict.lm(fitted, newdata = new, interval = "prediction")
pred <- cbind("names"= new, pred)
pred
#5
num_5 <- data.frame(duration2 <- 3)
names(num_5) <- "values"
#Predition intervals
pred <- predict.lm(fitted, newdata=num_5, interval = "prediction")
pred <- cbind(new, pred)
#Confidence intervals for the subpopulations
pred <- predict.lm(fitted, newdata=num_5, interval = "confidence")
pred <- cbind(new, pred)
num_5 <- data.frame(duration2 <- 3)
#Predition intervals
pred <- predict.lm(fitted, newdata=num_5, interval = "prediction")
pred <- cbind(new, pred)
pred
#Predition intervals
pred <- predict.lm(fitted, newdata = num_5, interval = "prediction")
#Confidence intervals for the subpopulations
pred <- predict.lm(fitted, newdata = num_5, interval = "confidence")
pred
#Predition intervals
pred <- predict.lm(fitted, newdata = new, interval = "prediction")
pred <- cbind(new, pred)
new <- data.frame(duration2 <- c(3,4,5))
names(new) <- "values"
#Predition intervals
pred <- predict.lm(fitted, newdata = new, interval = "prediction")
pred <- cbind(new, pred)
pred
num_5 <- data.frame(duration2 <- 3)
#Predition intervals
pred <- predict.lm(fitted, newdata = num_5, interval = "prediction")
pred
pred <- predict.lm(fitted, newdata = num_5, interval = "confidence")
pred
data <- read.delim("Burnout.dat", header = TRUE)
head(data)
data$burnout
data["Burnout2"] <= data$burnout == "Burnt Out"
data["Burnout2"] <- data$burnout == "Burnt Out"
data$Burnout2
data["Burnout2"] <- as.numeric(data$burnout == "Burnt Out")
data$Burnout2
head(data)
model_0 <- glm(burnout2 ~ loc + cope, data = data, family = "binomial")
model_0 <- glm(burnout2 ~ loc + cope, data = data, family = "binomial")
data["burnout2"] <- as.numeric(data$burnout == "Burnt Out")
model_0 <- glm(burnout2 ~ loc + cope, data = data, family = "binomial")
model_0
summary(model_0)
model_1 <- glm(burnout2 ~ loc + cope + teaching + research + pastoral, data = data, family = "binomial")
summary(model_1)
source('~/Documents/Spring 2016/STAT 3080/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
1 - binom.test(2,15,p=.3, alternative = "less")
(2,15,p=.3, alternative = "less")
binom.test(2, 15, p=.5, alternative = "less")
binom.test(2, 15, p=.3, alternative = "less")
1 - binom.test(2, 15, p=.3, alternative = "less")
binom.test(2, 15, p=.5, alternative = "less")
p_2 <- binom.test(2, 15, p=.3, alternative = "less")
p_2
p_2$p.value
1 - p_3$p.value
#Part 1
p_1 <- binom.test(2, 15, p=.5, alternative = "less")
#This outputs .003693, which is the probability of a type 1 error
#Part 2
p_2 <- binom.test(2, 15, p=.3, alternative = "less")
1 - p_2$p.value
#Our probability of a type 2 error is .1268
#Part 3
p_3 <- binom.test(2,15,p=.1, alternative = "less")
1 - p_3$p.value
pnorm((.4-.25)/sqrt(.25*.25/60), alternative = "greater")
help(pnorm)
dnorm(4)
dnorm(-4)
dnorm(0)
pnorm(4)
pnorm(5)
#Part 1
p <- (.4-.5)/sqrt((.5*.5)/60)
pnorm(p)
#This outputs .06066, the type 1 error probability.
#Part 2
p_2 <- (.4-.25)/sqrt(.25*.25/60)
pnorm(p_2, lower.tail = FALSE)
p_2
p_2 <- (.3-.25)/sqrt(.25*.25/60)
pnorm(p_2, lower.tail = FALSE)
p_2 <- (.26-.25)/sqrt(.25*.25/60)
pnorm(p_2, lower.tail = FALSE)
p_2 <- (.25 -.25)/sqrt(.25*.25/60)
pnorm(p_2)
p <- (.4-.5)/sqrt((.5*.5)/60)
pnorm(p)
#Problem 4
no.beers <- c(5,2,9,8,3,7,3,5,3,5)
bal <- c(.1,.03,.19,.12,.04,.095,.07,.06,.02,.05)
model <- lm(bal ~ no.beers)
summary(model)
source('~/Documents/Spring 2016/STAT 3080/Greenawald_Thomas_Modeling_HW.R', echo=TRUE)
sum(seq(1:100))
#Script to play around with various ideas for the grades app
setwd("C://Users/Student/Documents/UVA Grades App")
library(plyr)
library(dplyr)
library(ggplot2)
library(readxl)
data = read_excel("Grades.xlsx")
use = tbl_df(data)
middleton = filter(use, Instructor.Last.Name == "Middleton")
x1 <- ddply(middleton, .(Instructor.Last.Name, Period), summarize, A = sum(A))
x1
middleton
colnames(use)
middleton[1]
middleton[,1]
middleton[1,]
middleton$Course.GPA
source('~/Documents/UVA Grades App/back_end_tester.R', echo=TRUE)
x1 <- ddply(middleton, .(common_names), summarize,  A = sum(A))
x1
middleton = filter(use, Instructor.Last.Name == "Middleton")
x1 <- ddply(middleton, .(common_names), summarize,  A = sum(A))
x1
x1$common_names
middleton = filter(use, Instructor.Last.Name == "Middleton")
x1 <- ddply(middleton, by = common_names, summarize,  A = sum(A))
x1 <- ddply(middleton, "Period", summarize,  A = sum(A))
x1
source('~/Documents/UVA Grades App/back_end_tester.R', echo=TRUE)
x1 <- ddply(middleton, "Period", summarize,  A = sum(A))
source('~/Documents/UVA Grades App/back_end_tester.R', echo=TRUE)
x1
source('~/Documents/UVA Grades App/back_end_tester.R', echo=TRUE)
x1
x1 <- ddply(middleton, "Period", summarize, C= sum(C), A = sum(A), match = "all")
x1
x1 <- ddply(middleton, .(common_names), summarize, C= sum(C), A = sum(A))
x1
data = read_excel("Grades.xlsx")
use = tbl_df(data)
# Common names for the merge
common_names <- c(Instructor.Last.Name, Instructor.First.Name, Instructor.Middle.Name,
Instructor.Email, Course.Number, Title, Period)
middleton = filter(use, Instructor.Last.Name == "Middleton")
x1 <- ddply(middleton, .(common_names), summarize, C= sum(C), A = sum(A))
x1
Read in the data
data = read_excel("Grades.xlsx")
use = tbl_df(data)
# Common names for the merge
common_names <- .(Instructor.Last.Name, Instructor.First.Name, Instructor.Middle.Name,
Instructor.Email, Course.Number, Title, Period)
middleton = filter(use, Instructor.Last.Name == "Middleton")
x1 <- ddply(.data = middleton, .variables = common_names, summarize, C= sum(C), A = sum(A))
# Read in the data
data = read_excel("Grades.xlsx")
use = tbl_df(data)
# Common names for the merge
common_names <- .(Instructor.Last.Name, Instructor.First.Name, Instructor.Middle.Name,
Instructor.Email, Course.Number, Title, Period)
middleton = filter(use, Instructor.Last.Name == "Middleton")
x1 <- ddply(.data = middleton, .variables = common_names, summarize, C= sum(C), A = sum(A))
x1
data = read_excel("Grades.xlsx")
use = tbl_df(data)
# Common names for the merge
common_names <- .(Instructor.Last.Name, Instructor.First.Name, Instructor.Middle.Name,
Instructor.Email, Course.Number, Title, Period)
middleton = filter(use, Instructor.Last.Name == "Middleton")
# Merge common sections
x1 <- ddply(.data = middleton, .variables = common_names, summarize, A_plus = sum(A_plus),
A = sum(A), A_minus = sum(A_minus), B_plus = sum(B_plus), B = sum(B), B_minus = sum(B_minus),
C_plus = sum(C_plus), C = sum(C), C_minus = sum(C_minus), D_plus = sum(D_plus), D = sum(D),
D_minus = sum(D_minus), fail = sum(fail))
x1
x1$fail
# Common names for the merge
common_names <- .(Instructor.Last.Name, Instructor.First.Name, Instructor.Middle.Name,
Instructor.Email, Course.Number, Title, Period)
middleton = filter(use, Instructor.Last.Name == "Middleton")
# Merge common sections
row_merge <- function(in_data_frame){
return(ddply(.data = in_data_frame, .variables = common_names, summarize, A_plus = sum(A_plus),
A = sum(A), A_minus = sum(A_minus), B_plus = sum(B_plus), B = sum(B), B_minus = sum(B_minus),
C_plus = sum(C_plus), C = sum(C), C_minus = sum(C_minus), D_plus = sum(D_plus), D = sum(D),
D_minus = sum(D_minus), fail = sum(fail)))
}
middleton
middleton$Period
x1 <- row_merge(middleton)
x1
# Given a row within the grades data set, calculate the GPA
calc_GPA <- function(in_data_row){
# Calucate the total grade points earned in this row
earned <- 4*in_data_row$A_plus + 4*in_data_row$A + 3.7*in_data_row$A_minus +
3.3*in_data_row$B_plus + 3*in_data_row$B + 2.7*in_data_row$B_minus +
2.3*in_data_row$C_plus + 2*in_data_row$C + 1.7*in_data_row$C_minus +
1.3*in_data_row$D_plus + 1*in_data_row$D + .7*in_data_row$D_minus
# Calculate the maximum possible points that could be earned
total <- in_data_row$Tot - in_data_row$DR - in_data_row$W
# Return GPA (the ratio of these two). Note that number of credits will cancel
# out so it is not included in this computation
return(earned/total)
}
calc_GPA(middleton[,1])
x_1 <- middleton[,1]
x_1
x_1 <- middleton[1,]
x_1
calc_GPA(x_1)
test_row <- filter(use, Title == "Electronic Commerce", Period == "2011.F")
test_row
test_row$Course.GPA
calc_GPA(test_row)
calc_GPA <- function(in_data_row){
# Calucate the total grade points earned in this row
earned <- 4*in_data_row$A_plus + 4*in_data_row$A + 3.7*in_data_row$A_minus +
3.3*in_data_row$B_plus + 3*in_data_row$B + 2.7*in_data_row$B_minus +
2.3*in_data_row$C_plus + 2*in_data_row$C + 1.7*in_data_row$C_minus +
1.3*in_data_row$D_plus + 1*in_data_row$D + .7*in_data_row$D_minus
# Calculate the maximum possible points that could be earned
total <- in_data_row$Tot - in_data_row$DR - in_data_row$W
# Return GPA (the ratio of these two). Note that number of credits will cancel
# out so it is not included in this computation
return(round(earned/total),2)
}
calc_GPA(test_row)
calc_GPA <- function(in_data_row){
# Calucate the total grade points earned in this row
earned <- 4*in_data_row$A_plus + 4*in_data_row$A + 3.7*in_data_row$A_minus +
3.3*in_data_row$B_plus + 3*in_data_row$B + 2.7*in_data_row$B_minus +
2.3*in_data_row$C_plus + 2*in_data_row$C + 1.7*in_data_row$C_minus +
1.3*in_data_row$D_plus + 1*in_data_row$D + .7*in_data_row$D_minus
# Calculate the maximum possible points that could be earned
total <- in_data_row$Tot - in_data_row$DR - in_data_row$W
# Return GPA (the ratio of these two). Note that number of credits will cancel
# out so it is not included in this computation
return(round(earned/total,2))
}
calc_GPA(test_row)
