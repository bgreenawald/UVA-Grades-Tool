#1
fitted = lm(waiting2 ~ duration2)
#plot(fitted)
#The intercept (waiting2) is 34.95 while the slope of the model line
#10.78
#2
#Using the residuals vs. fitted graph, we can see that within each of
#our blobs, the points seem to be distributed randomly, but symmetrically
#about the dotted line, so our data does not seem to violate linearity or
#homoscedasticity. But looking at the qqplot of the residuals, we see
#that the residuals are not normal at the extremes, and thus violate this
#requirement of linear models, thus we conclude that this is not a relaible
#model of the data
#3
summary(fitted)
#We are testing the assumption the B=0 (where B represents the slope of the
#line of best fit in waiting2 ~ duration2). We want to test at a significance
#level of .05. So Ho: B=0 while Ha: B!=0. Using the summary function,
#we see that our p-value is 2.2e-16 which is smaller than .05 so we reject
#our null and conclude that there is a linear relationship between
#waiting time and duration of eruption.
#4
#Way one, we will use the summary function and look at the value
summary(fitted)
#which tells us that R^2 is .7887
#Way two is to extract it directly from the summary function
summary(fitted)$adj.r.squared
#and we get the same result
#For the third way, we will write a function that uses our linear model to calculate r^2
get_val <- function(x){
y <- (10.78*x) + 34.95
return(y)
}
#BUGGGG
waiting_2 <- sapply(duration2, get_val)
diff_sqaured <- (waiting_2-waiting2)^2
dist <- (waiting2 - mean(waiting2))^2
sum_dist <- sum(dist)
total_line_error <- sum(diff_sqaured)
r_2 <- sum_dist / total_line_error
#4
newdta = data.frame(waiting2=3)
model <- predict(fitted, newdata=newdta, interval = "predict")
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
waiting2
class(waiting2)
class(duration2)
predict(fitted, interval = "predict")
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
model
duration2
getwd()
getwd()
Cars93
head(Cars93)
source('~/Documents/Spring 2016/STAT 3080/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
source('~/Documents/Spring 2016/STAT 3080/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
head(Cars93)
model_0 <- lm(MPG.city ~ EngineSize+Weight+Passengers+Price, data = Cars93)
summary(model_0)
source('~/Documents/Spring 2016/STAT 3080/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
summary(model_1)
#We do the same with the next highest non-signficant p-value, Passengers
model_2 <- update(model_1, ~.-Passengers)
summary(model_2)
source('~/Documents/Spring 2016/STAT 3080/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
plot(model_3)
data <- read.delim("Burnout.dat", header = TRUE)
head(data)
library(psych)
dummy <- C(data$burnout, treatment)
dummy
model <- lm(data$burnout ~ data$loc + data$cope)
model
summary(model)
source('~/Documents/Spring 2016/STAT 3080/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
model_0 <- lm(log(MPG.city) ~ EngineSize+Weight+Passengers+Price, data = Cars93)
summary(model_0)
#and remove that variable from our model to create a new model.
model_1 <- update(model_0, ~.-EngineSize)
summary(model_1)
#We do the same with the next highest non-signficant p-value, Passengers
#with a p-value of .558
model_2 <- update(model_1, ~.-Passengers)
summary(model_2)
# We do the same with Price whose p-value is .257
model_3 <- update(model_2, ~.-Price)
summary(model_3)
#################
plot(model_3)
model_0 <- lm(log(log(MPG.city)) ~ EngineSize+Weight+Passengers+Price, data = Cars93)
summary(model_0)
#We do the same with the next highest non-signficant p-value, Passengers
#with a p-value of .558
model_2 <- update(model_1, ~.-Passengers)
summary(model_2)
#We will try and take the logarithm of the data to see if we get a better model
model_0 <- lm(log(log(MPG.city)) ~ EngineSize+Weight+Passengers+Price, data = Cars93)
summary(model_0)
#We do the same with the next highest non-signficant p-value, Passengers
#with a p-value of .558
model_2 <- update(model_0, ~.-Passengers)
summary(model_2)
#We choose the highest non-significant p-value (EngineSize with a p-value of .740)
#and remove that variable from our model to create a new model.
model_1 <- update(model_2, ~.-EngineSize)
summary(model_1)
# We do the same with Price whose p-value is .257
model_3 <- update(model_2, ~.-Price)
summary(model_3)
plot(model_3)
#1
pnorm((.4-.5)/sqrt((.5*.5)/60))
#This outputs .06066, the type 1 error probability.
#2
pnorm((.4-.25)/sqrt(.25*.25/60))
#Our probability of a type 2 error is .9999983
###########################################
library(MASS)
attach(geyser)
duration2<-duration[-299]
waiting2<-waiting[-1]
#1
fitted = lm(waiting2 ~ duration2)
#plot(fitted)
#The intercept (waiting2) is 34.95 while the slope of the model line
#10.78
#2
#Using the residuals vs. fitted graph, we can see that within each of
#our blobs, the points seem to be distributed randomly, but symmetrically
#about the dotted line, so our data does not seem to violate linearity or
#homoscedasticity. Looking at the qqplot of the residuals, we see
#that the residuals also appear to be approximately normal so our assumptions
#for linear models do hold.
#3
summary(fitted)
#We are testing the assumption the B=0 (where B represents the slope of the
#line of best fit in waiting2 ~ duration2). We want to test at a significance
#level of .05. So Ho: B=0 while Ha: B!=0. Using the summary function,
#we see that our p-value is 2.2e-16 which is smaller than .05 so we reject
#our null and conclude that there is a linear relationship between
#waiting time and duration of eruption.
#4
#Way one, we will use the summary function and look at the value
summary(fitted)
#which tells us that R^2 is .789
#Way two is to extract it directly from the summary function
summary(fitted)$adj.r.squared
#and we get the same result
#For the third way, we will write a function that uses our linear model to calculate r^2
get_val <- function(x){
y <- (10.78*x) + 34.95
return(y)
}
#Maybe a small bug?
waiting_2 <- sapply(duration2, get_val)
diff_sqaured <- (waiting_2-waiting2)^2
dist <- (waiting2 - mean(waiting2))^2
sum_dist <- sum(dist)
total_line_error <- sum(diff_sqaured)
r_2 <- total_line_error / sum_dist
1-r_2
#4,
new <- c(3,4,5)
predict.lm(fitted, newdata = new, interval = "prediction")
#4,
new <- c(3,4,5)
predict.lm(fitted, newdata = new, interval = "confidence")
#4,
new <- data.frame(3,4,5)
predict.lm(fitted, newdata = new, interval = "confidence")
#4,
new <- data.frame(3,4,5)
predict.lm(fitted, newdata = new, interval = "prediction")
#4,
new <- data.frame(waiting2 <- c(3,4,5))
predict.lm(fitted, newdata = new, interval = "prediction")
#4,
new <- data.frame(duration2 <- c(3,4,5))
predict.lm(fitted, newdata = new, interval = "prediction")
#4,
new <- data.frame(duration2 <- c(3,4,5))
pred <- predict.lm(fitted, newdata = new, interval = "prediction")
pred <- cbind(new, pred)
pred
#4,
new <- data.frame(duration2 <- c(3,4,5))
pred <- predict.lm(fitted, newdata = new, interval = "prediction")
pred <- cbind("names"= new, pred)
pred
new
#4,
new <- data.frame(duration2 <- c(3,4,5))
names(new) <- "names"
pred <- predict.lm(fitted, newdata = new, interval = "prediction")
pred <- cbind("names"= new, pred)
pred
#5
num_5 <- data.frame(duration2 <- 3)
names(num_5) <- "values"
#Predition intervals
pred <- predict.lm(fitted, newdata=num_5, interval = "prediction")
pred <- cbind(new, pred)
#Confidence intervals for the subpopulations
pred <- predict.lm(fitted, newdata=num_5, interval = "confidence")
pred <- cbind(new, pred)
num_5 <- data.frame(duration2 <- 3)
#Predition intervals
pred <- predict.lm(fitted, newdata=num_5, interval = "prediction")
pred <- cbind(new, pred)
pred
#Predition intervals
pred <- predict.lm(fitted, newdata = num_5, interval = "prediction")
#Confidence intervals for the subpopulations
pred <- predict.lm(fitted, newdata = num_5, interval = "confidence")
pred
#Predition intervals
pred <- predict.lm(fitted, newdata = new, interval = "prediction")
pred <- cbind(new, pred)
new <- data.frame(duration2 <- c(3,4,5))
names(new) <- "values"
#Predition intervals
pred <- predict.lm(fitted, newdata = new, interval = "prediction")
pred <- cbind(new, pred)
pred
num_5 <- data.frame(duration2 <- 3)
#Predition intervals
pred <- predict.lm(fitted, newdata = num_5, interval = "prediction")
pred
pred <- predict.lm(fitted, newdata = num_5, interval = "confidence")
pred
data <- read.delim("Burnout.dat", header = TRUE)
head(data)
data$burnout
data["Burnout2"] <= data$burnout == "Burnt Out"
data["Burnout2"] <- data$burnout == "Burnt Out"
data$Burnout2
data["Burnout2"] <- as.numeric(data$burnout == "Burnt Out")
data$Burnout2
head(data)
model_0 <- glm(burnout2 ~ loc + cope, data = data, family = "binomial")
model_0 <- glm(burnout2 ~ loc + cope, data = data, family = "binomial")
data["burnout2"] <- as.numeric(data$burnout == "Burnt Out")
model_0 <- glm(burnout2 ~ loc + cope, data = data, family = "binomial")
model_0
summary(model_0)
model_1 <- glm(burnout2 ~ loc + cope + teaching + research + pastoral, data = data, family = "binomial")
summary(model_1)
source('~/Documents/Spring 2016/STAT 3080/Greenawald_Thomas_ModelingHW.R', echo=TRUE)
1 - binom.test(2,15,p=.3, alternative = "less")
(2,15,p=.3, alternative = "less")
binom.test(2, 15, p=.5, alternative = "less")
binom.test(2, 15, p=.3, alternative = "less")
1 - binom.test(2, 15, p=.3, alternative = "less")
binom.test(2, 15, p=.5, alternative = "less")
p_2 <- binom.test(2, 15, p=.3, alternative = "less")
p_2
p_2$p.value
1 - p_3$p.value
#Part 1
p_1 <- binom.test(2, 15, p=.5, alternative = "less")
#This outputs .003693, which is the probability of a type 1 error
#Part 2
p_2 <- binom.test(2, 15, p=.3, alternative = "less")
1 - p_2$p.value
#Our probability of a type 2 error is .1268
#Part 3
p_3 <- binom.test(2,15,p=.1, alternative = "less")
1 - p_3$p.value
pnorm((.4-.25)/sqrt(.25*.25/60), alternative = "greater")
help(pnorm)
dnorm(4)
dnorm(-4)
dnorm(0)
pnorm(4)
pnorm(5)
#Part 1
p <- (.4-.5)/sqrt((.5*.5)/60)
pnorm(p)
#This outputs .06066, the type 1 error probability.
#Part 2
p_2 <- (.4-.25)/sqrt(.25*.25/60)
pnorm(p_2, lower.tail = FALSE)
p_2
p_2 <- (.3-.25)/sqrt(.25*.25/60)
pnorm(p_2, lower.tail = FALSE)
p_2 <- (.26-.25)/sqrt(.25*.25/60)
pnorm(p_2, lower.tail = FALSE)
p_2 <- (.25 -.25)/sqrt(.25*.25/60)
pnorm(p_2)
p <- (.4-.5)/sqrt((.5*.5)/60)
pnorm(p)
#Problem 4
no.beers <- c(5,2,9,8,3,7,3,5,3,5)
bal <- c(.1,.03,.19,.12,.04,.095,.07,.06,.02,.05)
model <- lm(bal ~ no.beers)
summary(model)
source('~/Documents/Spring 2016/STAT 3080/Greenawald_Thomas_Modeling_HW.R', echo=TRUE)
source('~/Documents/UVA Grades App/back_end_tester.R', echo=TRUE)
setwd("C:\User\Student\Documents\UVA Grades App")
setwd("C:/User/Student/Documents/UVA Grades App")
setwd("C://Users/Student/Documents/UVA Grades App")
install.packages("xlxs")
install.packages("XLConnect")
fall_2010 <- read.csv("Grades_Spring_2010.xlxs")
#Script to play around with various ideas for the grades app
setwd("C://Users/Student/Documents/UVA Grades App")
library(dplyr)
library(ggplot2)
library(XLConnect)
fall_2011 <- read.csv("Grades_Spring_2011.xlxs")
fall_2011 <- read.csv("Grades_Spring_2011")
fall_2011 <- read.xls("Grades_Fall_2011.xlsx")
library(gdata)
fall_2011 <- read.xls("Grades_Fall_2011.xlsx")
head(fall_2011)
str(fall_2011)
fall_2011_tbl <- tbl_df(fall_2011)
fall_2011_tbl
fall_2011_tbl$Course.GPA
filter(fall_2011_tbl, Course.GPA == max(Course.GPA))
x1 <- filter(fall_2011_tbl, Course.GPA == max(Course.GPA))
x1
x1$Course.GPA
x1$Subject
x1$Tot
x1$A
x1[1]
x1[1,]
x1[1,]
fall_2011[fall_2011$Course.Number == 5210]
fall_2011[fall_2011$Course.Number == 5210,]
x1 <- filter(fall_2011_tbl, Course.GPA == max(Course.GPA) && !is.na(A))
x1 <- filter(fall_2011_tbl, Course.GPA == max(Course.GPA) && !is.na(A))
x1
x1 <- filter(fall_2011_tbl, !is.na(A))
x2 <- filter(x1, Course.GPA == max(Course.GPA))
x2
x1 <- filter(fall_2011_tbl, !is.na(A))
x2 <- arrange(x1, Course.GPA)
x2
x2$Course.GPA
x2[1,]
x2[1,]$Title
x2[1,]$Subject
x2[1,]$C
x2[1,]$c.
x2[1,]$C.
x2[1,]$C..1
x2[1,]$A
x2[1,]$A..1
x2[1,]$B.
x2[1,]$B
x2[1,]$Tot
fall_2011 <- read.xls("Grades_Fall_2011.xlsx")
fall_2011_tbl <- tbl_df(fall_2011)
fall_2011_tbl$Period
fall_2011_tbl$Period
fall_2011 <- read.xls("Grades_Fall_2011.xlsx")
fall_2011_tbl <- tbl_df(fall_2011)
fall_2011_tbl$Period
fall_2011_tbl
fall_2015 <- read.xls("Grades_Fall_2015.xlsx")
fall_2015_tbl <- tbl_df(fall_2015)
fall_2015_tbl
fall_2015_tbl$Period
setwd("C://Users/Student/Documents/UVA Grades App")
library(dplyr)
library(ggplot2)
library(XLConnect)
wk = loadWorkbook("Grades.xlsx")
df = readWorksheet(wk, sheet="Sheet1")
use = tbl_df(df)
source('~/Documents/UVA Grades App/back_end_tester.R', echo=TRUE)
install.packages("readxl")
dir()
setwd("C://Users/Student/Documents/UVA Grades App")
library(dplyr)
library(ggplot2)
library(readxl)
data <- read_excel("Grades.xlsx")
use <- tbl_df(data)
warnings()
read_excel("Grades.xslx")
read_excel("Grades.xlsx")
x1 <- read_excel("Grades.xlsx")
x1
length(x1$`Instructor Last Name`)
install.packages("shiny")
library(shiny)
runExample("01-hello")
runExample("01_hello")
runApp("UVA Grades App")
runApp("UVA Grades App")
runApp("UVA Grades App")
setwd("C://Users/Student/Documents/UVA Grades App")
runApp("UVA Grade App")
runApp("UVA Grade App")
runApp("UVA Grades App")
runApp("app-files")
runApp("app-files")
source('~/Documents/UVA Grades App/back_end_tester.R', echo=TRUE)
head(uuse
)
head(use)
fall_2015 <- filter(use, Period == "2015.F")
fall_2015 = filter(fall_2015, !is.na(A))
fall_2015 = arrange(fall_2015, desc(Course.GPA))
fall_2015 = arrange(fall_2015, desc(Course GPA))
fall_2015 = arrange(fall_2015, desc("Course GPA"))
fall_2015 = arrange(fall_2015, desc(Course GPA))
args(read_excel)
#Script to play around with various ideas for the grades app
setwd("C://Users/Student/Documents/UVA Grades App")
library(dplyr)
library(ggplot2)
library(readxl)
data <- read_excel("Grades.xlsx")
use <- tbl_df(data)
head(use)
fall_2015 = filter(use, Period == "2015.F")
fall_2015 = filter(fall_2015, !is.na(A))
fall_2015 = arrange(fall_2015, desc(Course.GPA))
fall_2015 = select(fall_2015, Instructor.Last.Name, Subject, Course.Number, Title, Course.GPA, Tot)
head(fall_2015)
#Script to play around with various ideas for the grades app
setwd("C://Users/Student/Documents/UVA Grades App")
library(dplyr)
library(ggplot2)
library(readxl)
data = read_excel("Grades.xlsx")
use = tbl_df(data)
fall_2015 = filter(use, Period == "2015.F")
fall_2015 = filter(fall_2015, !is.na(A))
fall_2015 = arrange(fall_2015, desc(Course.GPA))
fall_2015 = select(fall_2015, Instructor.Last.Name, Subject, Course.Number, Title, Course.GPA, Tot)
head(fall_2015)
source('~/Documents/UVA Grades App/back_end_tester.R', echo=TRUE)
#Script to play around with various ideas for the grades app
setwd("C://Users/Student/Documents/UVA Grades App")
library(dplyr)
library(ggplot2)
library(readxl)
data = read_excel("Grades.xlsx")
use = tbl_df(data)
fall_2015 = filter(use, Period == "2015.F")
fall_2015 = filter(fall_2015, !is.na(A))
length(fall_2015)
#Script to play around with various ideas for the grades app
setwd("C://Users/Student/Documents/UVA Grades App")
library(dplyr)
library(ggplot2)
library(readxl)
data = read_excel("Grades.xlsx")
use = tbl_df(data)
fall_2015 = filter(use, Period == "2015.F")
fall_2015 = filter(fall_2015, !is.na(A))
length(fall_2015)
length(fall_2015$A)
#Script to play around with various ideas for the grades app
fall_2015 = filter(use, Period == "2015.F")
fall_2015 = filter(fall_2015, !is.na(A))
fall_2015 = arrange(fall_2015, Course.GPA)
fall_2015 = select(fall_2015, Instructor.Last.Name, Subject, Course.Number, Title, Course.GPA, Tot)
head(fall_2015)
length(fall_2015$A)
length(fall_2015$Subject)
michener = filter(use, Instructor.Last.Name == "Michener")
michener
michener_econ = filter(michener, Title == "Introduction to Econometrics")
ggplot(data = michener_econ, aes(Course.GPA ~ Period)) + geom_line()
ggplot(data = michener_econ, aes(Course.GPA ~ seq(1:7))) + geom_line()
michener_econ$Course.GPA
ggplot(data = michener_econ, aes(Course.GPA) + geom_point()
ggplot(data = michener_econ, aes(Course.GPA) + geom_point())
ggplot(data = michener_econ, aes(Course.GPA)) + geom_point()
ggplot(data = michener_econ, aes(x = Period, y = Course.GPA)) + geom_point()
ggplot(data = michener_econ, aes(x = Period, y = Course.GPA)) + geom_line()
ggplot(data = michener_econ, aes(x = Period, y = Course.GPA)) + geom_point() + geom_line()
ggplot(data = michener_econ, aes(x = seq(1:7), y = Course.GPA)) + geom_point() + geom_line()
michener_econ$Period
middleton = filter(use, Instructor.Last.Name == "Middleton" && Title == "Introduction to Management Accounting")
data = read_excel("Grades.xlsx")
use = tbl_df(data)
middleton = filter(use, Instructor.Last.Name == "Middleton")
middleton
middleton$Title
middleton = filter(use, Instructor.Last.Name == "Middleton")
ggplot(data = middleton, aes(x = seq(1:7), y = Course.GPA)) + geom_point() + geom_line()
middleton$Period
install.packages()
install.packages("plyr")
install.packages("plyr")
install.packages("plyr")
install.packages("plyr")
install.packages("plyr")
install.packages("plyr")
install.packages("plyr")
library(dplyr)
library(plyr)
library(ggplot2)
library(readxl)
